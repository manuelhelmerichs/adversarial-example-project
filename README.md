Neural networks are vulnerable to adversarial examples:
>*Given an input $x$ and any target classification $t$, it is possible to find a new input $x'$ that is similar to x but classified as $t$*

This repository hosts studies on a selection of adversarial example generation techniques:
- Carlini and Wagner attack [(CWA)](/techniques/cwa)
- Fast Gradient Sign Method [(FGSM)](/techniques/fgsm/)
- Jacobian-based saliency map attack [(JSMA)](/techniques/jsma)

The following papers have been used:

[1] T. B. Brown, D. Mané, A. Roy, M. Abadi, and J. Gilmer, ‘Adversarial Patch’. arXiv, May 16, 2018. Accessed: Nov. 13, 2023. [Online]. Available: http://arxiv.org/abs/1712.09665

[2] I. J. Goodfellow, J. Shlens, and C. Szegedy, ‘Explaining and Harnessing Adversarial Examples’. arXiv, Mar. 20, 2015. Accessed: Nov. 13, 2023. [Online]. Available: http://arxiv.org/abs/1412.6572

[3] B. Kolosnjaji et al., ‘Adversarial Malware Binaries: Evading Deep Learning for Malware Detection in Executables’, in 2018 26th European Signal Processing Conference (EUSIPCO), Rome: IEEE, Sep. 2018, pp. 533–537. doi: 10.23919/EUSIPCO.2018.8553214.

[4] A. Nguyen, J. Yosinski, and J. Clune, ‘Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images’. arXiv, Apr. 02, 2015. Accessed: Nov. 13, 2023. [Online]. Available: http://arxiv.org/abs/1412.1897

[5] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami, ‘The Limitations of Deep Learning in Adversarial Settings’. arXiv, Nov. 23, 2015. Accessed: Nov. 13, 2023. [Online]. Available: http://arxiv.org/abs/1511.07528

[6] C. Szegedy et al., ‘Intriguing properties of neural networks’. arXiv, Feb. 19, 2014. Accessed: Nov. 13, 2023. [Online]. Available: http://arxiv.org/abs/1312.6199

*This project is licensed under the Apache License 2.0 - see the LICENSE file for details.*
